{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "爬取url：https://www.sciencedirect.com/journal/journal-of-public-economics/vol/200/suppl/C\n",
    "\n",
    "爬取内容：一级页面上想抓每条记录的题目，作者，每个连接页面的二级页面想把关键词和英文摘要抓出来\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 17.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting lxml\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/cf/4d/6537313bf58fe22b508f08cf3eb86b29b6f9edf68e00454224539421073b/lxml-4.6.3-cp37-cp37m-manylinux1_x86_64.whl (5.5MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5MB 14.4MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-4.6.3\n"
     ]
    }
   ],
   "source": [
    "#如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\r\n",
    "!mkdir /home/aistudio/external-libraries\r\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries\r\n",
    "!pip install lxml -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \r\n",
    "# Also add the following code, \r\n",
    "# so that every time the environment (kernel) starts, \r\n",
    "# just run the following code: \r\n",
    "import sys \r\n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\r\n",
    "import re\r\n",
    "import requests\r\n",
    "import datetime\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import time #时间处理模块\r\n",
    "import argparse\r\n",
    "import copy\r\n",
    "import ast\r\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getinfo(url):\r\n",
    "    '''\r\n",
    "    请求url接口，返回soup信息\r\n",
    "    '''\r\n",
    "\r\n",
    "    headers = {\r\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36(KHTML, like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\r\n",
    "    requests.adapters.DEFAULT_RETRIES = 1\r\n",
    "    s = requests.session()    \r\n",
    "    s.keep_alive = False\r\n",
    "    re = s.get(url, headers=headers)\r\n",
    "    soup = BeautifulSoup(re.text, 'html.parser')\r\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "def saveInfoToFile1(volume1,volume2):\r\n",
    "    '''\r\n",
    "    解析数据，返回需要的数据articles\r\n",
    "    volume1:起始期目号\r\n",
    "    volume2:结束期目号+1\r\n",
    "    '''\r\n",
    "    articles = []\r\n",
    "    Keywords_title = \"Keywords\"\r\n",
    "    jelClassification_title = \"JEL\"\r\n",
    "    while(volume1!=volume2):\r\n",
    "        #url = 'https://www.sciencedirect.com/journal/journal-of-public-economics/vol/{}/suppl/C'\r\n",
    "        url = 'https://www.sciencedirect.com/journal/economia/vol/{}/issue/1'\r\n",
    "        url = url.format(str(volume1))\r\n",
    "        soup = getinfo(url)    \r\n",
    "        lis = soup.find_all('li', class_='js-article-list-item article-item u-padding-xs-top u-margin-l-bottom')\r\n",
    "        for li in lis:\r\n",
    "            article={}\r\n",
    "            article['title'] = li.find('span',class_='js-article-title').text\r\n",
    "            article['link'] = 'https://www.sciencedirect.com'+li.find('a', class_='anchor article-content-title u-margin-xs-top u-margin-s-bottom')['href']\r\n",
    "            try:\r\n",
    "                article['authors'] = li.find('div',class_='text-s u-clr-grey8 js-article__item__authors').text\r\n",
    "            except AttributeError:\r\n",
    "                article['authors'] = ''\r\n",
    "            soup1 = getinfo(article['link'])\r\n",
    "            try:\r\n",
    "                article['abstract']=soup1.findAll('div', class_=\"abstract author\")[0].find_all('div')[0].text\r\n",
    "            except:\r\n",
    "                article['abstract']=''\r\n",
    "            article['期刊页码']= soup1.findAll('div',class_='text-xs')[0].text\r\n",
    "            article['jelClassification'] = '无'\r\n",
    "            keywords_divs=soup1.findAll('div', class_=\"keywords-section\")\r\n",
    "            for keywords_div in  keywords_divs:           \r\n",
    "                #对当前节点前面的标签和字符串进行查找\r\n",
    "                div_title = keywords_div.find_all('h2')[0].text\r\n",
    "                if(Keywords_title in div_title):\r\n",
    "                    keywords_list = [keyword.text for keyword in keywords_div]\r\n",
    "                    article['keywords'] = ','.join(keywords_list[1:])\r\n",
    "                if(jelClassification_title in div_title):\r\n",
    "                    jelClassification_list = [keyword.text for keyword in keywords_div]\r\n",
    "                    article['jelClassification'] = ','.join(jelClassification_list[1:])\r\n",
    "            articles.append(article)\r\n",
    "            #time.sleep(np.random.randint(0,5)) #在2秒~10秒钟随意切换数值，降低爬取频率，防止被封杀\r\n",
    "        volume1 -= 1\r\n",
    "        \r\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveInfoToFile2(volume1,volume2,url_sub,page,url_origi):\r\n",
    "    '''\r\n",
    "    解析数据，返回需要的数据\r\n",
    "    volume1:起始期目号, 最大的\r\n",
    "    volume2:结束期目号+1\r\n",
    "    url_sub:网址中间类别\r\n",
    "    page：issue后面的最大数字\r\n",
    "    '''\r\n",
    "    #url_origi = 'https://www.sciencedirect.com/journal/{}/vol/{}/issue/{}'\r\n",
    "    articles = []\r\n",
    "    Keywords_title = \"Keywords\"\r\n",
    "    jelClassification_title = \"JEL\"\r\n",
    "    for volume_tmp in range(volume1,volume2,-1):\r\n",
    "        for page_tmp in range(page,0,-1):    \r\n",
    "            url = url_origi.format(url_sub,str(volume_tmp),str(page_tmp))\r\n",
    "            print(url)\r\n",
    "            soup = getinfo(url)    \r\n",
    "            lis = soup.find_all('li', class_='js-article-list-item article-item u-padding-xs-top u-margin-l-bottom')\r\n",
    "            for li in lis:\r\n",
    "                article={}\r\n",
    "                article['title'] = li.find('span',class_='js-article-title').text\r\n",
    "                article['link'] = 'https://www.sciencedirect.com'+li.find('a', class_='anchor article-content-title u-margin-xs-top u-margin-s-bottom')['href']\r\n",
    "                try:\r\n",
    "                    article['authors'] = li.find('div',class_='text-s u-clr-grey8 js-article__item__authors').text\r\n",
    "                except AttributeError:\r\n",
    "                    article['authors'] = ''\r\n",
    "                soup1 = getinfo(article['link'])\r\n",
    "                try:\r\n",
    "                    article['abstract']=soup1.findAll('div', class_=\"abstract author\")[0].find_all('div')[0].text\r\n",
    "                except:\r\n",
    "                    article['abstract']=''\r\n",
    "                article['期刊页码']= soup1.findAll('div',class_='text-xs')[0].text\r\n",
    "                article['jelClassification'] = '无'\r\n",
    "                keywords_divs=soup1.findAll('div', class_=\"keywords-section\")\r\n",
    "                for keywords_div in  keywords_divs:           \r\n",
    "                    #对当前节点前面的标签和字符串进行查找\r\n",
    "                    div_title = keywords_div.find_all('h2')[0].text\r\n",
    "                    if(Keywords_title in div_title):\r\n",
    "                        keywords_list = [keyword.text for keyword in keywords_div]\r\n",
    "                        article['keywords'] = ','.join(keywords_list[1:])\r\n",
    "                    if(jelClassification_title in div_title):\r\n",
    "                        jelClassification_list = [keyword.text for keyword in keywords_div]\r\n",
    "                        article['jelClassification'] = ','.join(jelClassification_list[1:])\r\n",
    "                articles.append(article)\r\n",
    "                #time.sleep(np.random.randint(0,5)) #在2秒~10秒钟随意切换数值，降低爬取频率，防止被封杀\r\n",
    "    \r\n",
    "    articles_df = pd.DataFrame(articles)\r\n",
    "    articles_df.to_csv('{}{}-{}.csv'.format(url_sub,volume1,volume2+1), encoding='utf-8')   \r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveInfoToFile3(volume1,volume2,url_sub,url_origi):\r\n",
    "    '''\r\n",
    "    解析数据，返回需要的数据\r\n",
    "    volume1:起始期目号, 最大的\r\n",
    "    volume2:结束期目号+1\r\n",
    "    url_sub:网址中间类别\r\n",
    "    url_c：TURE:url组后是C，否则是S1\r\n",
    "    '''\r\n",
    "    articles = []\r\n",
    "    Keywords_title = \"Keywords\"\r\n",
    "    jelClassification_title = \"JEL\"\r\n",
    "\r\n",
    "    for volume_tmp in range(volume1,volume2,-1):\r\n",
    "        for page_tmp in range(page,0,-1):\r\n",
    "            url = url_origi.format(url_sub,str(volume_tmp))\r\n",
    "            print(url)\r\n",
    "            soup = getinfo(url)    \r\n",
    "            lis = soup.find_all('li', class_='js-article-list-item article-item u-padding-xs-top u-margin-l-bottom')\r\n",
    "            for li in lis:\r\n",
    "                article={}\r\n",
    "                article['title'] = li.find('span',class_='js-article-title').text\r\n",
    "                article['link'] = 'https://www.sciencedirect.com'+li.find('a', class_='anchor article-content-title u-margin-xs-top u-margin-s-bottom')['href']\r\n",
    "                try:\r\n",
    "                    article['authors'] = li.find('div',class_='text-s u-clr-grey8 js-article__item__authors').text\r\n",
    "                except AttributeError:\r\n",
    "                    article['authors'] = ''\r\n",
    "                soup1 = getinfo(article['link'])\r\n",
    "                try:\r\n",
    "                    article['abstract']=soup1.findAll('div', class_=\"abstract author\")[0].find_all('div')[0].text\r\n",
    "                except:\r\n",
    "                    article['abstract']=''\r\n",
    "                article['期刊页码']= soup1.findAll('div',class_='text-xs')[0].text\r\n",
    "                article['jelClassification'] = '无'\r\n",
    "                keywords_divs=soup1.findAll('div', class_=\"keywords-section\")\r\n",
    "                for keywords_div in  keywords_divs:           \r\n",
    "                    #对当前节点前面的标签和字符串进行查找\r\n",
    "                    div_title = keywords_div.find_all('h2')[0].text\r\n",
    "                    if(Keywords_title in div_title):\r\n",
    "                        keywords_list = [keyword.text for keyword in keywords_div]\r\n",
    "                        article['keywords'] = ','.join(keywords_list[1:])\r\n",
    "                    if(jelClassification_title in div_title):\r\n",
    "                        jelClassification_list = [keyword.text for keyword in keywords_div]\r\n",
    "                        article['jelClassification'] = ','.join(jelClassification_list[1:])\r\n",
    "                articles.append(article)\r\n",
    "                #time.sleep(np.random.randint(0,5)) #在2秒~10秒钟随意切换数值，降低爬取频率，防止被封杀\r\n",
    "    \r\n",
    "    articles_df = pd.DataFrame(articles)\r\n",
    "    articles_df.to_csv('{}{}-{}.csv'.format(url_sub,volume1,volume2+1), encoding='utf-8')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveInfoToFile4(volume1,volume2,url_origi):\r\n",
    "    '''\r\n",
    "    解析数据，返回需要的数据\r\n",
    "    volume1:起始期目号, 最大的\r\n",
    "    volume2:结束期目号+1\r\n",
    "    '''\r\n",
    "    articles = []\r\n",
    "    Keywords_title = \"Keywords\"\r\n",
    "    jelClassification_title = \"JEL\"\r\n",
    "\r\n",
    "    for volume_tmp in range(volume1,volume2,-1):\r\n",
    "        for page_tmp in range(page,0,-1):\r\n",
    "            url = url_origi\r\n",
    "            print(url)\r\n",
    "            soup = getinfo(url)    \r\n",
    "            lis = soup.find_all('li', class_='js-article-list-item article-item u-padding-xs-top u-margin-l-bottom')\r\n",
    "            for li in lis:\r\n",
    "                article={}\r\n",
    "                article['title'] = li.find('span',class_='js-article-title').text\r\n",
    "                article['link'] = 'https://www.sciencedirect.com'+li.find('a', class_='anchor article-content-title u-margin-xs-top u-margin-s-bottom')['href']\r\n",
    "                try:\r\n",
    "                    article['authors'] = li.find('div',class_='text-s u-clr-grey8 js-article__item__authors').text\r\n",
    "                except AttributeError:\r\n",
    "                    article['authors'] = ''\r\n",
    "                soup1 = getinfo(article['link'])\r\n",
    "                try:\r\n",
    "                    article['abstract']=soup1.findAll('div', class_=\"abstract author\")[0].find_all('div')[0].text\r\n",
    "                except:\r\n",
    "                    article['abstract']=''\r\n",
    "                article['期刊页码']= soup1.findAll('div',class_='text-xs')[0].text\r\n",
    "                article['jelClassification'] = '无'\r\n",
    "                keywords_divs=soup1.findAll('div', class_=\"keywords-section\")\r\n",
    "                for keywords_div in  keywords_divs:           \r\n",
    "                    #对当前节点前面的标签和字符串进行查找\r\n",
    "                    div_title = keywords_div.find_all('h2')[0].text\r\n",
    "                    if(Keywords_title in div_title):\r\n",
    "                        keywords_list = [keyword.text for keyword in keywords_div]\r\n",
    "                        article['keywords'] = ','.join(keywords_list[1:])\r\n",
    "                    if(jelClassification_title in div_title):\r\n",
    "                        jelClassification_list = [keyword.text for keyword in keywords_div]\r\n",
    "                        article['jelClassification'] = ','.join(jelClassification_list[1:])\r\n",
    "                articles.append(article)\r\n",
    "                #time.sleep(np.random.randint(0,5)) #在2秒~10秒钟随意切换数值，降低爬取频率，防止被封杀\r\n",
    "    \r\n",
    "    articles_df = pd.DataFrame(articles)\r\n",
    "    articles_df.to_csv('{}{}-{}.csv'.format(url_sub,volume1,volume2+1), encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(__doc__)\r\n",
    "parser.add_argument(\"--url_issue\", type=ast.literal_eval, default=True, help=\"url have issue\")\r\n",
    "parser.add_argument(\"--url_c\", type=ast.literal_eval, default=True, help=\"end of url is C\")\r\n",
    "parser.add_argument(\"--url_s1\", type=ast.literal_eval, default=True, help=\"end of url is S1\")\r\n",
    "parser.add_argument(\"--url_sub\", type=str, default=None, help=\"url intermediate category.\")\r\n",
    "parser.add_argument(\"--page\", type=int, default=None, help=\"maximal page.\")\r\n",
    "parser.add_argument(\"--volume1\", type=int, default=None, help=\"beginning volume.\")i\r\n",
    "parser.add_argument(\"--volume2\", type=int, default=None, help=\" end volume.\")\r\n",
    "args = parser.parse_args()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\r\n",
    "   volume1,volume2 = args.volume1,args.volume2\r\n",
    "   a =saveInfoToFile1(volume1,volume2)\r\n",
    "   a_df = pd.DataFrame(a)\r\n",
    "   a_df.to_csv('文献信息{}-{}.csv'.format(volume1,volume2+1), encoding='utf-8')\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\r\n",
    "   volume1,volume2 = 22,14\r\n",
    "   a =saveInfoToFile1(volume1,volume2)\r\n",
    "   a_df = pd.DataFrame(a)\r\n",
    "   a_df.to_csv('文献信息{}-{}.csv'.format(volume1,volume2+1), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sciencedirect.com/journal/world-development/vol/28/issue/12\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.sciencedirect.com', port=443): Max retries exceeded with url: /science/article/pii/S0305750X00000784 (Caused by ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mssl_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mHAS_SNI\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mserver_hostname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m    869\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    719\u001b[0m             retries = retries.increment(\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.sciencedirect.com', port=443): Max retries exceeded with url: /science/article/pii/S0305750X00000784 (Caused by ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-482331e4e28b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0murl_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'world-development'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0murl_origi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.sciencedirect.com/journal/{}/vol/{}/issue/{}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msaveInfoToFile2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvolume1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvolume2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murl_sub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murl_origi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-cdd0dc2fa80c>\u001b[0m in \u001b[0;36msaveInfoToFile2\u001b[0;34m(volume1, volume2, url_sub, page, url_origi)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'authors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0msoup1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'link'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoup1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"abstract author\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3cc4db457150>\u001b[0m in \u001b[0;36mgetinfo\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_alive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.sciencedirect.com', port=443): Max retries exceeded with url: /science/article/pii/S0305750X00000784 (Caused by ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\r\n",
    "    volume1,volume2 = 28,27\r\n",
    "    page = 12\r\n",
    "    url_sub = 'world-development'\r\n",
    "    url_origi = 'https://www.sciencedirect.com/journal/{}/vol/{}/issue/{}'\r\n",
    "    saveInfoToFile2(volume1,volume2,url_sub,page,url_origi)\r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nohup python /home/aistudio/sciencedirect.py --url_c True --url_sub 'world-development' --volume1 140 --volume2 100 &\r\n",
    "nohup python /home/aistudio/sciencedirect.py --url_c True --url_sub 'world-development' --volume1 100 --volume2 70 &\r\n",
    "nohup python /home/aistudio/sciencedirect.py --url_c True --url_sub 'world-development' --volume1 70 --volume2 40 &\r\n",
    "nohup python /home/aistudio/sciencedirect.py --url_s1 True --url_sub 'world-development' --volume1 64 --volume2 63 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\r\n",
    "    \r\n",
    "    volume1,volume2 = args.volume1,args.volume2\r\n",
    "    page = args.page\r\n",
    "    url_sub = args.url_sub\r\n",
    "    saveInfoToFile2(volume1,volume2,url_sub,page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\r\n",
    "    if args.url_issue:\r\n",
    "        url_origi = 'https://www.sciencedirect.com/journal/{}/vol/{}/issue/{}'\r\n",
    "        saveInfoToFile2(args.volume1,args.volume2,args.url_sub,args.page,url_origi)\r\n",
    "    elif args.url_c:\r\n",
    "        url_origi = 'https://www.sciencedirect.com/journal/{}/vol/{}/suppl/C'\r\n",
    "        saveInfoToFile3(args.volume1,args.volume2,args.url_sub,url_origi)\r\n",
    "    elif args.url_s1:\r\n",
    "        url_origi = 'https://www.sciencedirect.com/journal/{}/vol/{}/suppl/S1'\r\n",
    "        saveInfoToFile3(args.volume1,args.volume2,args.url_sub,url_origi)\r\n",
    "    else:\r\n",
    "        url_origi = args.url_origi\r\n",
    "        saveInfoToFile4(args.volume1,args.volume2,url_origi)\r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!zip -r articles.zip /home/aistudio/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 合并csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "#待搜索的目录路径\r\n",
    "path = \"/home/aistudio/result_mid\"\r\n",
    "#待搜索的名称\r\n",
    "filename = \".csv\"\r\n",
    "#定义保存结果的数组\r\n",
    "result = []\r\n",
    "def findfiles(path,filename):\r\n",
    "    #在这里写下您的查找文件代码吧！\r\n",
    "    for maindir, subdir, file_name_list in os.walk(path):\r\n",
    "        for filename1 in file_name_list:\r\n",
    "            #print(filename1)\r\n",
    "            if filename in filename1:\r\n",
    "                apath = os.path.join(maindir, filename1)#合并成一个完整路径\r\n",
    "                #print(apath)\r\n",
    "                result.append(apath)\r\n",
    "    return result\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_url_sub(filename,url_sub):  \r\n",
    "    urls = findfiles('/home/aistudio/result_mid1', filename)\r\n",
    "    dfs = [pd.read_csv(url,header=0,index_col=0,encoding='utf-8') for url in urls]\r\n",
    "    for df in dfs:\r\n",
    "        df['url_sub'] = url_sub\r\n",
    "    outdf =pd.concat(dfs, axis = 0, ignore_index=True)\r\n",
    "    return outdf\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "outdf =pd.concat([add_url_sub(filename,url_sub) for  (filename,url_sub) in zip([\"world-development\",'economia','文献信息'],[\"world-development\",'economia','Journal of Public Economics'])], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outdf=outdf.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#获取当天的日期,并进行格式化,用于后面文件命名，格式:20200420\r\n",
    "today = datetime.date.today().strftime('%Y%m%d')  \r\n",
    "outdf.to_csv('/home/aistudio/result/文献信息{}.csv'.format(today), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf['url_sub'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = [pd.read_csv(url,header=0,index_col=0,encoding='utf-8') for url in urls]\r\n",
    "outdf =pd.concat(dfs, axis = 0, ignore_index=True)\r\n",
    "outdf.to_csv('/home/aistudio/result/文献信息.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10565 result/文献信息.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l result/文献信息.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26560 result/文献信息20210714.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l result/文献信息20210714.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",title,link,authors,abstract,期刊页码,jelClassification,keywords,url_sub\r\n",
      "0,Editorial: Human capital and human capability,https://www.sciencedirect.com/science/article/pii/S0305750X97100146,Amartya Sen,,\"Volume 25, Issue 12, December 1997, Pages 1959-1961\",无,,world-development\r\n",
      "1,“Unanticipated consequences” in anti-poverty programs,https://www.sciencedirect.com/science/article/pii/S0305750X9700106X,Robert Klitgaard,\"Many anti-poverty programs have unanticipated consequences. A typology is provided, and one of its categories is illustrated with examples: insufficient attention to the economics of the institutions that implement anti-poverty programs. This in turn leads to practical suggestions for improving these institutions, especially through better information and incentives.\",\"Volume 25, Issue 12, December 1997, Pages 1963-1972\",无,\"Poverty,public management,incentives,adjustment\",world-development\r\n",
      "2,Testing the limits of privatization: Argentine railroads,https://www.sciencedirect.com/science/article/pii/S0305750X97000922,Ravi Ramamurti,\"Just how far can privatization be pushed, and with what consequences? Based on a study of Argentine railroads, this paper concludes that even large, unprofitable firms in developing countries faced with market failures can be privatized, but the gains from doing so depend on how badly the state enterprise was performing to begin with, and the potential for introducing competition in the market and for the market. The broader lesson is that when both market failures and government failures are present, a public-private solution is preferable to a purely private or a purely public solution. Privatization is not a panacea but potentially a palliative, when it comes to reducing subsidies or coping with regulatory failures.\",\"Volume 25, Issue 12, December 1997, Pages 1973-1993\",无,\"privatization,regulation,railroads,infrastructure,Argentina,Latin America\",world-development\r\n",
      "3,An ecological economic approach to forest and biodiversity conservation: The case of vanuatu,https://www.sciencedirect.com/science/article/pii/S0305750X97001010,Luca Tacconi,\"The paper presents an ecological economic approach to forest and biodiversity conservation. Constructivist philosophy of social science is critically reviewed and adopted as the methodological basis of the approach. The paradigmatic basis of ecological economics is discussed and a model of human behavior suited to ecological economics is presented. This model accounts for altruistic, normative/affective, and irrational behavior, as well as the role of institutions. A discussion of participatory land use planning, which also accounts for intergenerational equity issues, completes the presentation of the ecological economic approach. Its application to the conservation of forests and biodiversity in Vanuatu demonstrates the operational applicability of the proposed approach.\",\"Volume 25, Issue 12, December 1997, Pages 1995-2008\",无,\"constructivism,participation,protected areas,land use,Pacific,Vanuatu\",world-development\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 result/文献信息20210714.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>期刊页码</th>\n",
       "      <th>jelClassification</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Post-Conflict Reconstruction and the Challenge...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Graciana del Castillo</td>\n",
       "      <td>Countries in post-conflict transitions have to...</td>\n",
       "      <td>Volume 29, Issue 12, December 2001, Pages 1967...</td>\n",
       "      <td>无</td>\n",
       "      <td>Latin America,El Salvador,political economy,po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One Kind of Freedom: Poverty Dynamics in Post-...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Michael R. Carter, Julian May</td>\n",
       "      <td>A 1993 South African living standards survey d...</td>\n",
       "      <td>Volume 29, Issue 12, December 2001, Pages 1987...</td>\n",
       "      <td>无</td>\n",
       "      <td>income dynamics,poverty measures,South Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Is Devolution Democratization?</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Nandini Sundar</td>\n",
       "      <td>Recent attempts at introducing new forms of go...</td>\n",
       "      <td>Volume 29, Issue 12, December 2001, Pages 2007...</td>\n",
       "      <td>无</td>\n",
       "      <td>devolution,social capital,state,forestry,Asia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Manufacture of Popular Perceptions of Scar...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Lyla Mehta</td>\n",
       "      <td>This paper critically examines some narratives...</td>\n",
       "      <td>Volume 29, Issue 12, December 2001, Pages 2025...</td>\n",
       "      <td>无</td>\n",
       "      <td>Asia,India,water scarcity,dams,narratives of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Institutional Pluralism and Housing Delivery: ...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Bishwapriya Sanyal, Vinit Mukhija</td>\n",
       "      <td>This paper demonstrates how institutional plur...</td>\n",
       "      <td>Volume 29, Issue 12, December 2001, Pages 2043...</td>\n",
       "      <td>无</td>\n",
       "      <td>Asia,India,housing,conflict,NGOs,slum redevelo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  Post-Conflict Reconstruction and the Challenge...   \n",
       "1           1  One Kind of Freedom: Poverty Dynamics in Post-...   \n",
       "2           2                     Is Devolution Democratization?   \n",
       "3           3  The Manufacture of Popular Perceptions of Scar...   \n",
       "4           4  Institutional Pluralism and Housing Delivery: ...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.sciencedirect.com/science/article/...   \n",
       "1  https://www.sciencedirect.com/science/article/...   \n",
       "2  https://www.sciencedirect.com/science/article/...   \n",
       "3  https://www.sciencedirect.com/science/article/...   \n",
       "4  https://www.sciencedirect.com/science/article/...   \n",
       "\n",
       "                             authors  \\\n",
       "0              Graciana del Castillo   \n",
       "1      Michael R. Carter, Julian May   \n",
       "2                     Nandini Sundar   \n",
       "3                         Lyla Mehta   \n",
       "4  Bishwapriya Sanyal, Vinit Mukhija   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Countries in post-conflict transitions have to...   \n",
       "1  A 1993 South African living standards survey d...   \n",
       "2  Recent attempts at introducing new forms of go...   \n",
       "3  This paper critically examines some narratives...   \n",
       "4  This paper demonstrates how institutional plur...   \n",
       "\n",
       "                                                期刊页码 jelClassification  \\\n",
       "0  Volume 29, Issue 12, December 2001, Pages 1967...                 无   \n",
       "1  Volume 29, Issue 12, December 2001, Pages 1987...                 无   \n",
       "2  Volume 29, Issue 12, December 2001, Pages 2007...                 无   \n",
       "3  Volume 29, Issue 12, December 2001, Pages 2025...                 无   \n",
       "4  Volume 29, Issue 12, December 2001, Pages 2043...                 无   \n",
       "\n",
       "                                            keywords  \n",
       "0  Latin America,El Salvador,political economy,po...  \n",
       "1      income dynamics,poverty measures,South Africa  \n",
       "2  devolution,social capital,state,forestry,Asia,...  \n",
       "3  Asia,India,water scarcity,dams,narratives of s...  \n",
       "4  Asia,India,housing,conflict,NGOs,slum redevelo...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/aistudio/result/文献信息.csv',encoding='utf-8')\r\n",
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['url_sub'] = df['link'].apply(lambda x: x.split('/')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>url_sub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  url_sub\n",
       "1  https://www.sciencedirect.com/science/article/...  science\n",
       "2  https://www.sciencedirect.com/science/article/...  science\n",
       "3  https://www.sciencedirect.com/science/article/...  science\n",
       "4  https://www.sciencedirect.com/science/article/...  science\n",
       "5  https://www.sciencedirect.com/science/article/...  science"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1:5,['link','url_sub']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# https://link.springer.com/journal/10018/volumes-and-issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys \r\n",
    "sys.path.append('/home/aistudio/external-libraries')\r\n",
    "import json\r\n",
    "import re\r\n",
    "import requests\r\n",
    "import datetime\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import time #时间处理模块\r\n",
    "import argparse\r\n",
    "import copy\r\n",
    "import ast\r\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_headers():\r\n",
    "    user_agent = [\r\n",
    "        \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\r\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\r\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0\",\r\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 3.0.30729; .NET CLR 3.5.30729; InfoPath.3; rv:11.0) like Gecko\",\r\n",
    "        \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\",\r\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\r\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\r\n",
    "        \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11\",\r\n",
    "        \"Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11\",\r\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\r\n",
    "        \"Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5\",\r\n",
    "        \"Mozilla/5.0 (iPod; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5\",\r\n",
    "        \"Mozilla/5.0 (iPad; U; CPU OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5\",\r\n",
    "        \"Mozilla/5.0 (Linux; U; Android 2.3.7; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\",\r\n",
    "        \"MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GRJ22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\",\r\n",
    "        \"Opera/9.80 (Android 2.3.4; Linux; Opera Mobi/build-1107180945; U; en-GB) Presto/2.8.149 Version/11.10\",\r\n",
    "        \"Mozilla/5.0 (Linux; U; Android 3.0; en-us; Xoom Build/HRI39) AppleWebKit/534.13 (KHTML, like Gecko) Version/4.0 Safari/534.13\",\r\n",
    "        \"Mozilla/5.0 (BlackBerry; U; BlackBerry 9800; en) AppleWebKit/534.1+ (KHTML, like Gecko) Version/6.0.0.337 Mobile Safari/534.1+\",\r\n",
    "        \"Mozilla/5.0 (hp-tablet; Linux; hpwOS/3.0.0; U; en-US) AppleWebKit/534.6 (KHTML, like Gecko) wOSBrowser/233.70 Safari/534.6 TouchPad/1.0\",\r\n",
    "        \"Mozilla/5.0 (SymbianOS/9.4; Series60/5.0 NokiaN97-1/20.0.019; Profile/MIDP-2.1 Configuration/CLDC-1.1) AppleWebKit/525 (KHTML, like Gecko) BrowserNG/7.1.18124\",\r\n",
    "        \"Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0; HTC; Titan)\",\r\n",
    "        \"UCWEB7.0.2.37/28/999\",\r\n",
    "        \"NOKIA5700/ UCWEB7.0.2.37/28/999\",\r\n",
    "        \"Openwave/ UCWEB7.0.2.37/28/999\",\r\n",
    "        \"Mozilla/4.0 (compatible; MSIE 6.0; ) Opera/UCWEB7.0.2.37/28/999\",\r\n",
    "        \"Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25\"\r\n",
    "    ]\r\n",
    "\r\n",
    "    headers = {\r\n",
    "        \r\n",
    "        \r\n",
    "        \r\n",
    "        'User-Agent': random.choice(user_agent),\r\n",
    "        \r\n",
    "    }\r\n",
    "\r\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getinfo(url):\r\n",
    "    '''\r\n",
    "    请求url接口，返回soup信息\r\n",
    "    '''\r\n",
    "    try:\r\n",
    "        re = requests.get(url, headers=get_headers())\r\n",
    "        \r\n",
    "        soup = BeautifulSoup(re.text, 'html.parser')\r\n",
    "        return soup\r\n",
    "    except Exception as e:\r\n",
    "        print(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveInfoToFile(volume1,volume2,url_sub,page1,page2):\r\n",
    "    articles = []\r\n",
    "    Keywords_title = \"Keywords\"\r\n",
    "    jelClassification_title = \"JEL Classification\"\r\n",
    "    for volume_tmp in range(volume1,volume2,-1):\r\n",
    "        for page_tmp in range(page1,page2,-1):    \r\n",
    "            url = url_origi.format(url_sub,str(volume_tmp),str(page_tmp))\r\n",
    "            print(url)\r\n",
    "            time.sleep(np.random.randint(1,3))\r\n",
    "            soup = getinfo(url)\r\n",
    "\r\n",
    "            page_journey =  soup.find('h1', class_=\"u-mb-8\").text\r\n",
    "\r\n",
    "            lis = soup.find_all('li', class_='c-list-group__item')\r\n",
    "            for li in lis:\r\n",
    "                article={}\r\n",
    "                article['title'] = li.find('h3',class_='c-card__title').text.strip()\r\n",
    "                article['link'] = li.find('a')['href']\r\n",
    "                creator = li.find_all('li',itemprop='creator')\r\n",
    "                article['authors'] = []\r\n",
    "                for author in creator:\r\n",
    "                    article['authors'].append(author.find('span',itemprop='name').text)\r\n",
    "                article['authors']=','.join(article['authors'])\r\n",
    "                article['page'] =  page_journey + ','+ li.find_all('li',class_=\"c-meta__item\")[-1].text\r\n",
    "                time.sleep(np.random.randint(1,3))\r\n",
    "                soup1 = getinfo(article['link'])\r\n",
    "                try:\r\n",
    "                    article['abstract']=soup1.findAll('div', class_=\"c-article-section__content\")[0].text\r\n",
    "                except:\r\n",
    "                    article['abstract']=''\r\n",
    "\r\n",
    "                keywords_list=[]\r\n",
    "                jels_list=[]\r\n",
    "                try:\r\n",
    "                    subjects = soup1.find_all('li', class_=\"c-article-subject-list__subject\")\r\n",
    "                except:\r\n",
    "                    article['keywords']=''\r\n",
    "                    article['jelClassification']=''\r\n",
    "                    articles.append(article)\r\n",
    "                    continue\r\n",
    "                for subject in subjects:\r\n",
    "                    #对当前节点前面的标签和字符串进行查找\r\n",
    "                    subject_title= subject.find_previous('h3', class_=\"c-article__sub-heading\")\r\n",
    "                    if(Keywords_title in subject_title):\r\n",
    "                        keywords_list.append(subject.text)\r\n",
    "                    if(jelClassification_title in subject_title):\r\n",
    "                        jels_list.append(subject.text)\r\n",
    "\r\n",
    "                article['keywords'] = ','.join(keywords_list)\r\n",
    "                article['jelClassification'] = ','.join(jels_list)    \r\n",
    "\r\n",
    "                articles.append(article)\r\n",
    "    \r\n",
    "    articles_df = pd.DataFrame(articles)\r\n",
    "    articles_df['url_sub'] = url_sub\r\n",
    "    articles_df.to_csv('/home/aistudio/result_mid/{}-{}-{}.csv'.format(url_sub,volume1,volume2+1), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://link.springer.com/journal/10018/volumes-and-issues/23-3\n",
      "https://link.springer.com/journal/10018/volumes-and-issues/23-2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\r\n",
    "    url_origi = 'https://link.springer.com/journal/{}/volumes-and-issues/{}-{}'\r\n",
    "    #saveInfoToFile(volume1,volume2,url_sub,page1,page2)\r\n",
    "    saveInfoToFile(23,22,'10018',3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "https://academic.oup.com/wber/article-abstract/35/2/287/5607661?redirectedFrom=fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'https://academic.oup.com/wber/article-abstract/35/2/287/5607661?redirectedFrom=fulltext'\r\n",
    "soup = getinfo(url)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sanitation,urbanization,religion,culture'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ','.join([keyword.text for keyword in soup.findAll('a', class_=\"kwd-part kwd-main\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I12 - Health Behavior,O15 - Human Resources; Human Development; Income Distribution; Migration,Z12 - Religion'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll('div', class_=\"article-metadata\")[0].text.strip()[4:].replace('\\n',',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
